# 3.4 Congestion control [spec]

This specification provides a solution to deal with network congestion in the IOTA network. The congestion control algorithm described in this file decides which messages should be processed and gossiped to node's neighbors.


## 3.4.1 Summary

Every network has to deal with its intrinsic limited resources in terms of bandwidth and node capabilities (CPU and storage). In this document, we present a congestion control algorithm to regulate the influx of messages in the network with the goal of maximizing throughput (messages/bytes per second) and minimizing delays. Furthermore, the following requirements must be satisfied:

*   _Consistency_. If a message is written by one honest node, it shall be written by all honest nodes within some delay bound.
*   _Fairness_. Nodes can obtain a share of the available throughput depending on their access mana. Throughput is shared in such a way that an attempt to increase the allocation of any node necessarily results in the decrease in the allocation of some other node with an equal or smaller allocation (max-min fairness).
*   _Security_. Malicious nodes shall be unable to interfere with either of the above requirements.

Further information can be found in our a paper [Access Control for Distributed Ledgers in the Internet of Things: A Networking Approach](https://arxiv.org/abs/2005.07778).


### 3.4.1.2 Proposal

Our proposal has three core components: 
*   A scheduling algorithm which ensures fair access for all nodes according to their access Mana.
*   A TCP-inspired algorithm for decentralized rate setting to utilize the bottleneck while preventing delays.
*   A blacklisting policy to ban malicious behavior.

### 3.4.1.3 Prerequisites

_Node identity_. We require node accountability where each message is associated with the node ID of its issuing node.

_Access mana_. The congestion control module has knowledge of the reputation of all nodes (access mana) in order to fairly share the available throughput. Without access mana the network would be subject to Sybil attacks, which would incentivise even honest actors to artificially increase its own number of nodes.

## 3.4.2 Congestion control algorithm

### 3.4.2.1 Outbox management

Once the message has successfully passed the message parser checks _and_ is solid, it is enqueued into the outbox for scheduling (see Section 2.4.x Data Flow). The outbox is logically split into several queues, each one corresponding to a different message's issuer. In this section, we describe the operations of message enqueuing (and dequeuing) into (from) the outbox.

The enqueuing mechanism includes the following components:

*   _Classification_. The mechanism identifies the queue where the message belongs to according to the node ID of the message issuer.
*   _Message enqueuing_. The message is actually enqueued, queue is sorted by message timestamps in increasing order and counters are updated (e.g., counters for the total number of bytes in the queue).
*   _Message drop_. In some circumstances, due to network congestion or to ongoing attacks, some messages shall be dropped to guarantee bounded delays and isolate attacker's messages. Specifically, a node shall drop messages in two situations: (i) since buffers are of a limited size, if the total number of bytes in all queues exceeds a certain threshold, new incoming messages are dropped; (ii) to guarantee the security of the network, if a certain queue exceeds a given threshold, new incoming packets from that specific node ID will be dropped.

The dequeue mechanism includes the following components:

*   _Queue selection_. A queue is selected according to round robin scheduling algorithm. In particular, we use a modified version of the deficit round robin (DRR) algorithm, and we describe it in Section 3.4.2.2.
*   _Message dequeuing_. The first message of the queue is dequeued, and counters are updated.
*   _Scheduler management_. Scheduler counters and pointers are updated.

### 3.4.2.2 Scheduler

The most critical task is the scheduling algorithm which must guarantee that, for an honest node `node`, the following requirements will be met:
* `node`'s messages will not accumulate indefinitely at any node (i.e., starvation is avoided), so the _consistency_ requirement will be ensured.
* `node`'s fair share of the network resources are allocated to it, guaranteeing the _fairness_ requirement.
* malicious nodes sending above their allowed rate will not interrupt `node`'s throughput, fulfilling the _security_ requirement.

We remind the reader that the above requirements are described in Section 3.4.1.

Although nodes in our setting are capable of more complex and customised behaviour than a typical router in a packet-switched network, our scheduler must still be lightweight and scalable due to the potentially large number of nodes requiring differentiated treatment. It is estimated that over 10,000 nodes operate on the Bitcoin network, and we expect that an even greater number of nodes are likely to be present in the IoT setting. For this reason, we adopt a scheduler based on [Deficit Round Robin](https://ieeexplore.ieee.org/document/502236) (DRR) (the Linux implementation of the [FQ-CoDel packet scheduler](https://tools.ietf.org/html/rfc8290), which is based on DRR, supports anywhere up to 65535 separate queues).

The DRR scans all non-empty queues in sequence. When a non-empty queue is selected, its priority counter (called _deficit_) is incremented by a certain value (called _quantum_). Then, the value of the deficit counter is a maximal amount of bytes that can be sent at this turn: if the deficit counter is greater than the weight of the message at the head of the queue, this message can be scheduled and the value of the counter is decremented by this weight. In our implementation, the quantum is proportional to node's access mana and we add a cap on the maximum deficit that a node can achieve to keep the network latency low. It is also important to mention that the weight of the message can be assigned in such a way that specific messages can be prioritized (low weight) or penalized (large weight); by default, in our mechanism the weight is proportional to the message size measured in bytes.

Here a fundamental remark: _the network manager sets up a desired maximum (fixed) rate_ `SCHEDULING_RATE` (measured in bits per second) _at which messages will be scheduled._ This rate mostly depends on the degree of decentralization desired: e.g., a larger rate leads to higher throughput but would leave behind slower devices which will fall out of sync.

### 3.4.2.3 Rate setting

If all nodes always had messages to issue, i.e., if nodes were continuously willing to issue new messages, the problem of rate setting would be very straightforward: nodes could simply operate at a fixed, assured rate, sharing the total throughput according to the percentage of access mana owned. The scheduling algorithm would ensure that this rate is enforceable, and that increasing delays or dropped messages are only experienced by misbehaving node. However, it is unrealistic that all nodes will always have messages to issue, and we would like nodes to better utilise network resources, without causing excessive congestion and violating any requirement.

We propose a rate setting algorithm inspired by TCP â€” each node employs [additive increase, multiplicative decrease](https://https://epubs.siam.org/doi/book/10.1137/1.9781611974225) (AIMD) rules to update their issuance rate in response to congestion events. In the case of distributed ledgers, all message traffic passes through all nodes, contrary to the case of traffic typically found in packet switched networks and other traditional network architectures. Under these conditions, local congestion at a node is all that is required to indicate congestion elsewhere in the network. This observation is crucial, as it presents an opportunity for a congestion control algorithm based entirely on local traffic.

Our rate setting algorithm outlines the AIMD rules employed by each node to set their issuance rate. Rate updates take place each time a message is scheduled. Node `node` sets its own local additive-increase variable `localIncrease(node)` based on its access mana and on a global increase rate parameter `RATE_SETTING_INCREASE`. An appropriate choice of `RATE_SETTING_INCREASE` ensures a conservative global increase rate which does not cause problems even when many nodes increase their rate simultaneously. Nodes wait `RATE_SETTING_PAUSE` seconds after a global multiplicative decrease parameter `RATE_SETTING_DECREASE`, during which there are no further updates made, to allow the reduced rate to take effect and prevent multiple successive decreases. At each update, `node` checks how many of its own messages are in its outbox queue, and responds with a multiplicative decrease if this number is above a threshold, `backoff(node)`, which is proportional to `node`'s access mana. If the number of `node`'s messages in the outbox is below the threshold, `node`'s issuance rate is incremented by its local increase variable `localIncrease(node)`.

### 3.4.2.4 Message blocking and blacklisting

If an incoming message made the outbox total buffer size to exceed its maximum capacity `MAX_BUFFER`, the same message would be dropped. In our analysis, we set buffers to be large enough to accommodate traffic from all honest nodes.

Furthermore, to mitigate spamming actions from malicious nodes, we add an additional constraint: if `node`'s access mana-scaled queue length (i.e., queue length divided by node's access mana) exceeds a given threshold `MAX_QUEUE`, any new incoming packet from `node` will be dropped, hence the node is blacklisted. The attacker is blacklisted for a certain time `BLACKLIST_TIME` during which no messages issued by `node` can be added to the outbox. Please note that it is still possible to receive message from the attacker through solidification requests, which is important in order to guarantee the consistency requirement.

## 3.4.3 Algorithmic details

### 3.4.3.1 Protocol parameters

In line with the previous section, all nodes know the following global variables:

*   `float SCHEDULING_RATE`: minimum time interval between two scheduled messages
*   `float RATE_SETTING_INCREASE`: global additive increase parameter
*   `float RATE_SETTING_DECREASE`: global multiplicative decrease parameter (larger than 1)
*   `int RATE_SETTING_PAUSE`: waiting time before next rate's update after a backoff
*   `int MAX_BUFFER`: maximum buffer size (in bytes)
*   `float MAX_QUEUE`: maximum access mana-scaled inbox length
*   `float MAX_DEFICIT`: maximum cap for accumulated deficit
*   `float MAX_RATE`: maximum theoretical rate at which a node can be allowed to issue messages
*   `int BLACKLIST_TIME`: time interval during which no messages from blacklisted nodes are added to the outbox

### 3.4.3.2 Local variables

*   `float ownRate`: issuance rate of `ownID` according to the rate setter
*   `list activeNode`: updated list of nodes having at least one message in the outbox queue
*   `queue bufferQueue`: actual outbox queue where messages are ready to be scheduled
*   `nodeID nextID`: pointer to the specific queue where next message can be scheduled from
*   `list mana`: contains the _up-to-date_ (at the time the vector is used) value of the access mana given a certain `nodeId`. The way in which `mana` is updated is out of the scope of this spec, and further information can be found in Section 4.3.
*   `float backoff`: local threshold for rate setting
*   `float localIncrease`: local additive increase parameter
*   `list blacklisted`: list of flags indicating if a specific nodeId is blacklisted

### 3.4.3.3 Built-in functions

* `Len()`
* `Append()`
* `Weight()`
* `Sort()`

### 3.4.3.4 Pseudocode

The congestion control algorithm follows the _parser checks_ in the data flow: when a new message `msg` arrives to the scheduler, the function `Enqueue(msg)` will be triggered in order to properly add `msg` to the outbox. Furthermore, at regular intervals `1/SCHEDULING_RATE`, the function `Schedule()` picks a new message that has to be gossiped to neighbors and to be added to the local ledger; simultaneously, `RateSetting()` adjusts the message generation rate of `ownID` according to the network congestion.

### `Enqueue(msg)`

The function `Enqueue(msg)` adds a new message `msg` into the outbox and updates the list of active nodes accordingly. The check on the buffer size (`Len(bufferQueue) < MAX_BUFFER`) may be performed by the parser checker.
```vbnet
### upon arrival of a new message msg (having passed parser checks) ###

FUNCTION Enqueue(msg)

### ADD BLACKLISTING FILTER !!! ###

    IF Len(bufferQueue) < MAX_BUFFER
        nodeID = msg.nodeID
        IF activeNode[nodeID] != NULL
            # other messages from nodeID are already in the queue
            nodeQueue = activeNode*[nodeID]
            IF (Len(nodeQueue) + Len(msg))/mana[nodeID] < MAX_QUEUE
                # append msg
                Append(bufferQueue, msg)
                Sort(bufferQueue, timestamp)
            ELSE
                # discard msg and blacklist nodeID
                blacklisted[nodeID] = TRUE
        ELSE
            # no other messages for nodeID are present in the buffer
            Append(activeNode, nodeID)
            activeNode[nodeID].deficit = MAX_DEFICIT
            Append(bufferQueue, msg)
```

### `RateSetting()`

The function `RateSetting()` updates the rate `ownRate` at which messages can be issued by the node. The maximum value that `ownRate` can reach is `MAX_RATE`. At the bootstrap, the value of `ownRate` is initialized with `RATE_SETTING_INCREASE` times the proportion of access mana owned by the node. Finally, the local variable `haltUpdate` is initialized to _0_.
```vbnet
FUNCTION RateSetting()
    # update issueing rate if no recent backoff
    IF haltUpdate > 0
        haltUpdate -= 1
    ELSE IF ownRate < MAX_RATE
        # retrieve message queue of the same node
        IF Len(bufferQueue[ownID]) / mana[ownID] > backoff
            ownRate = ownRate / RATE_SETTING_DECREASE
            haltUpdate = RATE_SETTING_PAUSE
        ELSE
            ownRate += RATE_SETTING_INCREASE * mana[ownID] / Sum(mana)
```

### `Schedule()`

At regular intervals, i.e., every `1/SCHEDULING_RATE` time units, the function `Schedule()` selects the next message to gossip and process, if at least one message exists in `bufferQueue`. Otherwise, it returns `NULL` and the scheduling slot is missed.
```vbnet
### periodically every 1/SCHEDULING_RATE time units ###

FUNCTION msg = Schedule()
    IF Len(bufferQueue) > 0
        # point to a nodeId with enough deficit
        WHILE activeNode[nextID].deficit < Weight(bufferQueue[nextID].head):
            activeNode[nextID].deficit += mana[nextID]
            IF activeNode[nextID].deficit > MAX_DEFICIT
                activeNode[nextID].deficit = MAX_DEFICIT
            nextID++
        # msg represents the message in the outbox
        msg = bufferQueue[nextID].head
        activeNode[nextID].deficit -= Weight(msg)
        IF activeNode[nextID].deficit < 0
            activeNode[nextID] = 0
        # remove scheduled message from queue
        Remove(bufferQueue[nextID].head)
        # update list of active nodes
        IF Len(bufferQueue[nextID]) == 0
            Remove(activeNode[nextID])
        
        ### ADD BLACKLISTING PAUSE !!! ###
        
        # self update of sending rate
        RateSetting()
        RETURN msg
    ELSE
        RETURN NULL
```

### 3.4.3.5 Data structures

In this section we describe the architectural components used to handle the outbox queue. This is mainly done through two data structures:

*   _activeNode_. It is a list which includes the node IDs of the nodes having at least one message in the outbox queue. Each node ID in the list points to its oldest message in the outbox buffer `bufferQueue`.
*   _bufferQueue_. It is the actual outbox queue. We can build overlapping virtual queues (indicated by colors in the figure) to represent different queues per node. This data structure has a limited fixed size (in terms of number of messages), and removes old messages according to a FIFO policy.

![](https://i.imgur.com/Ek5eGef.png)

Other information about the hardware implementation of similar scheduling algorithms can be found at [this link](https://ieeexplore.ieee.org/document/642834).

## 3.4.4 Under research

*   _Dynamic scheduling rate._ In the current proposal, the throughput is preset by the network manager. This value takes into account nodesâ€™ hardware as well as bandwidth capacity. Hardware improvement or protocol optimizations will not result in a performance improvement if the network manager does not change the throughput parameter `SCHEDULING_RATE`. We are currently investigating a way to dynamically adapt the throughput according to the network and protocol characteristics based on neighbors health state.
*   _Self backoff._ We are investigating a better self back off condition to keep an average lower buffer length which can improve delays and decrease the probability that a honest node is blacklisted
*   _Blacklisting._ Condition for blacklisting should be improved to only punish truly malicious behavior. Furthermore, what to exactly do when a node is blacklisted is subject to further research: ideally, a node should be discarding all new incoming messages from the blacklisted node for a limited time and add this filter to the parser checks; other actions can also be taken into account.
*   _Deficit._ How is the deficit counter initialized? We have two options: (i) set the deficit counter equal to `MAX_DEFICIT` or (ii) equal to _0_. In the former, nodes will build deficit by not doing anything which may open up attack vectors; in the latter, transactions may propagate with large delays as nodes start build deficit only when a message arrives. An approach to mitigate this is to make `MAX_DEFICIT` proportional to node's access mana.
*   _Rate setter updates_. Should the rate setter be called only when node's messages are scheduled?
* _Max buffer capacity._ As a future direction, we will investigate how to properly design buffer capacity to achieve an optimal tradeoff between efficiency (low delay) and consistency (few message drops).
