---


---

<h1 id="congestion-control-spec">Congestion control [spec]</h1>
<p>This specification is part of <a href="https://coordicide.iota.org/">Coordicide</a>.<br>
Further information can be found in a related <a href="https://https://arxiv.org/pdf/2005.07778.pdf">research paper</a>.</p>
<h2 id="summary">Summary</h2>
<p>Every network has to deal with its intrinsic limited resources in terms of bandwidth and node capabilities (CPU and storage). In this document, we present a congestion control algorithm to regulate the influx of messages in the network with the goal of maximizing throughput and minimizing delays. Furthermore, the following requirements must be satisfied:</p>
<ul>
<li><em>Consistency</em>. If a message is written by one correct node, it should be written by all correct nodes within some delay bound.</li>
<li><em>Fairness</em>. All nodes should get a fair share of throughput, achieving max-min fairness in the number of messages issued by each node during congestion, weighted by mana.</li>
<li><em>Security</em>. Malicious nodes should be unable to interfere with either of the above requirements.</li>
</ul>
<h3 id="current-implementation">Current implementation</h3>
<p>The module described in this specification does not replace any existing mechanism.</p>
<h3 id="proposal">Proposal</h3>
<p>Our proposal has three core components: (i) a scheduling algorithm which ensures fair access for all nodes according to their mana; (ii) a TCP-inspired algorithm for decentralized rate setting to utilize the bottleneck while preventing delays; (iii) a blacklisting policy to ban malicious behavior.</p>
<h3 id="prerequisites">Prerequisites</h3>
<p><em>Node identity</em>. We require node accountability where each message is associated with the global identifier of its issuing node.</p>
<p><em>Mana</em>. The congestion control module has knowledge of the reputation of all nodes (mana) in order to fairly share the total throughput. Without mana the network is subject to Sybil attacks, and the thoughput is equally shared between nodes.</p>
<h2 id="congestion-control-algorithm">Congestion control algorithm</h2>
<h3 id="outbox-management">Outbox management</h3>
<p>Once the message has successfully passed the parser checks, it is enqueued into the outbox for scheduling. In this section, we describe the operations of message enqueuing (and dequeuing) into (from) the outbox.</p>
<p>The enqueue mechanism is made of the following:</p>
<ul>
<li><em>Classification</em>. The mechanism identifies the queue where the message belongs to according to the node ID of the message issuer. It is also possible to use a private salt to avoid DoS attacks (although it may be unnecessary if a rate control mechanism is in place).</li>
<li><em>Message enqueue</em>. The message is actually enqueued, queue is sorted by message timestamps and counters are updated (e.g., counters for the total number of bytes in the queue).</li>
<li><em>Message drop</em>. Buffers are of a limited size. If the total number of bytes in all queues exceeds a certain threshold, some messages should be dropped. In our analysis, we assume buffers to be large enough to accommodate traffic from all honest nodes. However, we consider a second constraint per queue: if a certain queue exceeds a given threshold, new incoming packets from that specific node ID will be dropped.</li>
</ul>
<p>The dequeue mechanism is made of the following:</p>
<ul>
<li><em>Queue selection</em>. A queue is selected according to a modified version of the deficit round robin (DRR) algorithm.</li>
<li><em>Message dequeue</em>. The last message of the queue is dequeued, and counters are updated.</li>
<li><em>Scheduler management</em>. Deficit counters and round robin pointers are updated.</li>
</ul>
<p>The most critical task is the actual scheduling algorithm which must guarantee that, for a honest node <em>m</em>, the following requirements will be met: (i) <em>m</em>’s messages will not become backlogged at any node, so <em>consistency</em> will be ensured; (ii) <em>m</em>’s fair share of the network resources are allocated to it, guaranteeing <em>fairness</em>; (iii) malicious nodes sending above their allowed rate will not interrupt <em>m</em>’s throughput, fulfilling the <em>security</em> requirement.</p>
<p>Although nodes in our setting are capable of more complex and customised behaviour than a typical router in a packet-switched network, our scheduler must still be lightweight and scalable due to the potentially large number of nodes requiring differentiated treatment. It is estimated that over 10,000 nodes operate on the Bitcoin network, and we expect that an even greater number of nodes are likely to be present in the IoT setting. For this reason, we adopt a scheduler based on <a href="https://ieeexplore.ieee.org/document/502236">DRR</a>. The Linux implementation of the <a href="https://tools.ietf.org/html/rfc8290">FQ-CoDel packet scheduler</a>, which is based on DRR, supports anywhere up to 65535 separate queues.</p>
<p>Here a fundamental remark: <em>the network manager sets up a desired maximum (fixed) rate 1/r at which messages will be scheduled.</em> This rate mostly depends on the degree of decentralization desired: e.g., a larger rate leads to higher throughput but would leave behind slower devices which will fall out of sync. A way to dynamically adjust this rate is currently under investigation.</p>
<h3 id="rate-setting">Rate setting</h3>
<p>If all nodes always had messages to issue, the problem of rate setting would be very straightforward: nodes could simply operate at a fixed, assured rate. The scheduling algorithm ensures that this rate is enforceable, and that increasing delays or dropped messages are only experienced by misbehaving node. However, it is highly unlikely that all nodes will always have messages to issue, and we would like nodes to better utilise network resources, without causing excessive congestion and violating requirements.</p>
<p>Our rate setting algorithm is inspired by TCP — each node employs <a href="https://https://epubs.siam.org/doi/book/10.1137/1.9781611974225">additive increase, multiplicative decrease</a> (AIMD) rules to update their issue rate in response to congestion events. In the case of distributed ledgers, all messages traffic passes through all nodes, contrary to the case of traffic typically found in packet switched networks and other traditional network architectures. Under these conditions, local congestion at a node is all that is required to indicate congestion elsewhere in the network. This observation is crucial, as it presents an opportunity for a congestion control algorithm based entirely on local traffic.</p>
<p>Our rate setting algorithm outlines the AIMD rules employed by each node to set their issue rate. Rate updates take place each time a message is scheduled. Node <em>m</em> sets its own local additive-increase parameter <em>α(m)</em> based on the global increase rate <em>A</em>, and its mana. An appropriate choice of <em>A</em> ensures a conservative global increase rate which does not cause problems even when many nodes increase their rate simultaneously. Nodes wait <em>τ</em> seconds after a multiplicative decrease (with global parameter <em>β</em>), during which there are no further updates made, to allow the reduced rate to take effect and prevent multiple successive decreases. At each update, node <em>m</em> checks how many of its own messages are in its outbox queue, and responds with a multiplicative decrease if this number is above a threshold, <em>L(m)</em>, which is proportional to <em>m</em>’s reputation. If the number of <em>m</em>’s messages in outbox is below the threshold, <em>m</em>’s issue rate is incremented by its local increase parameter <em>α(m)</em>.</p>
<h3 id="rate-setting">Message drop</h3>
<p>If an incoming message made the outbox total buffer size exceeding its maximum capacity, the same message would be dropped. In our analysis, we assume buffers to be large enough to accommodate traffic from all honest nodes. As a future direction, we will investigate how to properly design buffer capacity to achieve an optimal tradeoff between efficiency (low delay) and consistency (low message drops).</p>
<p>To mitigate spamming actions from malicious nodes, we add an additional constraint: if <em>m</em>'s reputation-scaled queue length exceeds (i.e., queue length divided by node's reputation) a given threshold <em>B</em>, any new incoming packet from <em>m</em> will be dropped, hence the node is blacklisted.</p>

<h2 id="algorithm">Algorithm</h2>
<h3 id="protocol-variables">Protocol variables</h3>
<p>In line with the previous section, all nodes know the following global variables:</p>
<ul>
<li><code>float r</code>: time interval between two scheduled messages</li>
<li><code>float A</code>: global additive increase parameter</li>
<li><code>float β</code>: global multiplicative decrease parameter</li>
<li><code>float τ</code>: time to wait after a self backoff (rate decrease)</li>
<li><code>float B</code>: threshold for blacklisting malicious behavior</li>
</ul>
<h3 id="node-variables">Node variables</h3>
<ul>
<li><code>list activeNode</code>: updated list of nodes having at least one message in the outbox queue</li>
<li><code>queue bufferQueue</code>: actual outbox queue where messages are ready to be scheduled</li>
<li><code>pointer rr_pointer</code>: pointer to <em>activeNode</em> used by the round robin scheduler</li>
<li><code>database nodeDB</code>: database with information related to mana and deficit counter</li>
<li><code>float L</code>: local threshold for rate setting</li>
<li><code>float α</code>: local additive increase parameter</li>
</ul>
<h3 id="pseudocode">Pseudocode</h3>
<h4 id="updatedeficitcounternodeid-value"><code>updateDeficitCounter(nodeId, value)</code></h4>
<p>This procedure updates the deficit counter for a given node. The deficit counter can be either incremented by a value proportional to node’s mana or decremented when a message is scheduled. Information concerning all nodes is stored in <em>nodeDB</em>. It makes sense to include in this data structure both mana and deficit counters.</p>
<pre><code>if value == -1:
    q = nodeDB[nodeId].mana
    nodeDB[nodeId].dc += q   
else:                        // decrease deficit when a message is scheduled
    nodeDB[nodeId].dc -= value
</code></pre>
<h4 id="dc--getdeficitcounternodeid"><code>dc = getDeficitCounter(nodeId)</code></h4>
<p>This function returns the value of the deficit counter given a certain <em>nodeId</em>.</p>
<pre><code>    return nodeDB[nodeId].dc
</code></pre>
<h4 id="messagedrop"><code>messageDrop()</code></h4>
<p>The buffer management rule is equivalent to the Longest Queue Drop policy. The node with the largest occupied buffer, relative to itWe use a FIFO policy to drop messages in a fixed-size outbox buffer. More efficient policies areputation, is identified, and the last received transaction is dropped.</p>
<pre><code>	max_queue_len = 0
	for nodeId in activeNodWe use a FIFO policy to drop messages in a fixed-size outbox buffer. More efficient policies are currently under research.</p>
<pre><code>    msgDrop = bufferQueue.head()
    
    if msgDrop.nodePtr == NULL:
        if activeNode*[nodeId].size()/nodeDB[nodeId].mana &gt; max_queue_len
	        max_queue_len = activeNode*[nodeId].size()/nodeDB[nodeId].mana
	        longestQueue = nodeId

    activeNode[longestQueue].tail().remov[msgDrop.nodeId].remove()
    else:
        activeNode[msgDrop.nodeId] = msgDrop.nodePtr  // assign the node pointer
        
    bufferQueue.dequeue()
</code></pre>
<h4 id="getadditiveparam"><code>getAdditiveParam()</code></h4>
<p>Get local additive parameter with up-to-date mana.</p>
<pre><code>// upon changes of node's mana (or initialization)
 
α = A * nodeDB[self].mana/sum(nodeDB.mana)
</code></pre>
<h4 id="ratesettingnodequeue"><code>rateSetting(nodeQueue)</code></h4>
<p>Update the rate at which messages can be issued by the node. During the bootstrap, the value of λ is initialized with the output of <code>getAdditiveParam()</code>.</p>
<pre><code>if nodeQueue.size() &gt; L:
    λ = λ * β
    wait τ seconds for next update
else:
    λ = λ + getAdditiveParam()
</code></pre>
<h4 id="enqueuemsg"><code>enqueue(msg)</code></h4>
<p>The procedure <em>enqueue(msg)</em> adds a new message <em>msg</em> into the outbox and updates the list of active nodes accordingly.</p>
<pre><code>// upon arrival of a new message msg (having passed processor filters)

if activeNode[msg.nodeId] != NULL:
    nodeQueue = activeNode*[msg.nodeId]   // (virtual) queue of messages from msg.nodeId
    bufferQueue.enqueue(msg)
    nodeQueue.tail() = bufferQueue.tail()
else:
    activeNode.add(msg.nodeId)
    bufferQueue.enqueue(msg)
    activeNode[msg.nodeId] = bufferQueue.tail()

if bufferQueue.size() &gt; MAX_LEN:
    messageDrop()
</code></pre>
<h4 id="msg--schedule"><code>msg = schedule()</code></h4>
<p>At regular intervals, schedule the next message through DRR if there is at least one message in <em>bufferQueue</em>. Otherwise, return <code>NULL</code>.</p>
<pre><code>// every r time units

if bufferQueue.size() &gt; 0:
    
    while getDeficitCounter(activeNode[rr_pointer]) &lt; 1:
        rr_pointer := rr_pointer.next()
        updateDeficitCounter(activeNode[rr_pointer], -1)
    
    msg = activeNode*[rr_pointer]  // msg represents the message in the outbox
        
    if msg.nodePtr == NULL:
        activeNode[rr_pointer].remove()
    else:
        activeNode[rr_pointer].ptr = msg.nodePtr  // assign the node pointer 
    
    updateDeficitCounter(activeNode[rr_pointer].nodeId, 1)
    activeNode*[rr_pointer].remove()   // remove the oldest message of the chosen node
    
    nodeQueue = activeNode*[self]   // (virtual) queue of messages from the node itself
    rateSetting(nodeQueue)

    return msg
else:
    return NULL
</code></pre>
<h3 id="data-structures">Data structures</h3>
<p>In this section we describe the architectural components used to handle the outbox queue. This is mainly done through two data structures:</p>
<ul>
<li><em>activeNode</em>. It is a list which includes the node IDs of the nodes having at least one message in the outbox queue. Each node ID in the list points to its oldest message in the outbox buffer <code>bufferQueue</code>.</li>
<li><em>bufferQueue</em>. It is the actual outbox queue. We can build overlapping virtual queues (indicated by colors in the figure) to represent different queues per node. This data structure has a limited fixed size (in terms of number of messages), and removes old messages according to a FIFO policy.</li>
</ul>
<p><img src="https://i.imgur.com/Ek5eGef.png" alt=""></p>
<p>Other information about the hardware implementation of similar scheduling algorithms can be found at <a href="https://ieeexplore.ieee.org/document/642834">this link</a>.</p>
<h2 id="attack-vectors">Attack vectors</h2>
<h3 id="spammer-nodes">Spammer nodes</h3>
<p>Assume that a node sends messages at a rate which is larger than what is determined by the <code>rateSetting()</code> procedure. We have performed extensive simulations and verified that the scheduler algorithm is robust enough against such an attack and still guarantees fairness. In fact, the number of undisseminated messages issued by the malicious node is constantly growing, as a result of large backlogs in its messages in the buffers of non-malicious nodes. Excessively full inbox buffers are therefore a simple means of detecting malicious behaviour, and action can be taken to remove the malicious node from the network. Dealing with detected attackers is currently under investigation.</p>
<h2 id="under-research">Under research</h2>
<ul>
<li><em>Dynamic scheduling rate.</em> In the current proposal, the throughput is preset by the network manager. This value takes into account nodes’ hardware as well as bandwidth capacity. Hardware improvement or protocol optimizations will not result in a performance improvement if the network manager does not change the throughput parameter <em>r</em>. We are currently investigating a way to dynamically adapt the throughput according to the network and protocol characteristics based on neighbors health state.</li>
<li><em>Spammer nodesMessage drop policy.</em> AssumWe that a node sends messages at a rate which is larger than what is determined by the <code>rateSetting()</code> procedure. We have performed extensive simulations and verified that the scheduler algorithm is robust enough against such an attack and still guarantees fairness. In fact, the number of undisseminated messages issued by the malicious node is constantly growing, as a result of large backlogs in its messages in the buffers of non-malicious nodes. Excessively full inbox buffers are therefore a simple means of detecting malicious behaviour, and action can be taken to remove the malicious node from the network. Dealing with detected attackers is currently under investigationve shown that a malicious node cannot manage increase its own traffic share. However, it can successfully inflate the outbox queue with its messages - which will, however, be scheduled with very low priority. We require a way to detect and censor such malicious behavior to avoid higher priority messages to be dropped. An idea is to drop messages in FIFO order depending on queue length (per node).</li>
</ul>

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE3Nzg3NDAzNDksMjE4NDgyMjA5XX0=
-->
